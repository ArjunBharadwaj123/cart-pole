# CartPole Q-Learning ğŸ§ ğŸ¤–

CartPole Q-Learning is a reinforcement learning project that trains an agent to balance a pole on a moving cart using the **Q-learning algorithm**.  
The agent interacts with the CartPole environment, learns from rewards, and gradually improves its policy through trial and error.  
To handle the environmentâ€™s **continuous state space**, observations are discretized into bins, enabling learning with a tabular Q-table.

---

## ğŸš€ Features

- Implements **Q-learning** from scratch in Python  
- Solves the classic **CartPole control problem**  
- Handles **continuous state spaces** via discretization  
- Uses an **epsilon-greedy policy** for exploration vs. exploitation  
- Learns an optimal policy over multiple training episodes  
- Easily configurable hyperparameters (learning rate, discount factor, epsilon)

---

## ğŸ§© Tech Stack

**Language:** Python  
**Libraries:** Gymnasium / OpenAI Gym, NumPy  
**Concepts:** Reinforcement Learning, Q-Learning, Markov Decision Processes

---

## âš™ï¸ Setup & Run Locally

### 1. Clone the repository

git clone [https://github.com/<your-username>/cartpole-qlearning.git](https://github.com/ArjunBharadwaj123/cart-pole)

cd cart-pole


### 2. Run the training script

python driverCode.py


---

## ğŸ§  How It Works

1. The CartPole environment provides four continuous observations:
   - Cart position  
   - Cart velocity  
   - Pole angle  
   - Pole angular velocity  

2. These observations are **discretized into bins** to form a finite state space.

3. A **Q-table** is initialized and updated using the Q-learning update rule:

Q(s, a) = Q(s, a) + Î± [ r + Î³ max Q(s', a') âˆ’ Q(s, a) ]


4. The agent selects actions using an **epsilon-greedy policy**, balancing exploration and exploitation.

5. Over many episodes, the agent learns a policy that maximizes cumulative reward by keeping the pole balanced longer.

---

## ğŸ“ˆ Results

After sufficient training, the agent is able to consistently balance the pole for extended periods, demonstrating convergence toward an effective control policy.

---

## ğŸ“š Future Improvements

- Replace state discretization with function approximation  
- Implement **Deep Q-Networks (DQN)**  
- Add reward and performance visualizations  
- Perform systematic hyperparameter tuning

---

## âœï¸ Author

**Arjun Bharadwaj**  
Computer Science, University of Maryland
